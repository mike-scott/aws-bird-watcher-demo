{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae920b4-3a29-4fc0-8b15-c1c1ddc0e318",
   "metadata": {},
   "source": [
    "# Setup dependencies\n",
    "\n",
    "**Before you begin, you will need your API Token from AI Hub.**  \n",
    "To get this value, log into AI Hub and copy it from [here](https://app.aihub.qualcomm.com/account/).  \n",
    "Then open the `aihub_api_token.txt` file and paste it there.  \n",
    "Use CTRL-S to save the file and close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6d0b2-e491-4ee7-ae20-7f893d660b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "![ ! -s \"aihub_api_token.txt\" ] && echo \"ERROR!! Place your AI Hub token in aihub_api_token.txt file and re-run this block!\"\n",
    "\n",
    "# Make sure we remove PyPi Ultralytics\n",
    "!{sys.executable} -m pip uninstall -y ultralytics\n",
    "\n",
    "# AI Hub uses some patches for YOLO\n",
    "# https://github.com/quic/ai-hub-models/blob/v0.18.0/qai_hub_models/models/yolov8_det/model.py#L92\n",
    "# Clone the Ultralytics repo\n",
    "![ ! -d \"ultralytics\" ] && git clone https://github.com/ultralytics/ultralytics -b v8.3.34\n",
    "# TFLite doesn't support quantized division, so convert to multiply\n",
    "!sed -i 's|/ 2|* 0.5|g' ultralytics/ultralytics/utils/tal.py\n",
    "# Boxes and scores have different scales, so return separately\n",
    "!sed -i 's/y = torch.cat((dbox, cls.sigmoid()), 1)/return (dbox, cls.sigmoid())/g' ultralytics/ultralytics/nn/modules/head.py\n",
    "!cd ultralytics/ && git diff\n",
    "# Install patched ultralytics\n",
    "!{sys.executable} -m pip install {os.getcwd()}/ultralytics/\n",
    "\n",
    "# Fix error:\n",
    "# ImportError: libGL.so.1: cannot open shared object file: No such file or directory\n",
    "!sudo apt-get install -q -y libgl1\n",
    "\n",
    "# Fix error:\n",
    "# TensorFlow SavedModel: export failure âŒ 167.2s: libusb-1.0.so.0: cannot open shared object file: No such file or directory\n",
    "!sudo apt-get install -q -y libusb-1.0-0-dev\n",
    "\n",
    "# Install and configure qai-hub for Quantize steps\n",
    "!{sys.executable} -m pip install -q qai_hub\n",
    "![ -s \"aihub_api_token.txt\" ] && qai-hub configure --api_token $(cat aihub_api_token.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409370d0-3d92-4438-ab46-9964557de2bd",
   "metadata": {},
   "source": [
    "# Configure settings\n",
    "\n",
    "The following block sets the configuration for building the model.\n",
    "\n",
    "NOTES:\n",
    "- To setup the **FULL** training session change `SAMPLE_ONLY = False`.  Training will take a long time once all of the classes are enabled.\n",
    "- If you run tests with `SAMPLE_ONLY = True` and then change to `SAMPLE_ONLY = False`, **you will need to re-run \"Download and prepare the dataset\"**.\n",
    "- **If you restart the kernel this block MUST always be re-run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3beb4cf-97af-491d-a343-6175e6dfb97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the dataset to 5 classes in order to test training quickly, change to False for full training\n",
    "SAMPLE_ONLY = True\n",
    "if SAMPLE_ONLY:\n",
    "    CLASS_FILTER = [17, 36, 47, 68, 73]\n",
    "else:\n",
    "    CLASS_FILTER = []\n",
    "\n",
    "# Dataset settings\n",
    "DATASET_NAME = \"CUB_200_2011\"\n",
    "DATASET_FILENAME = DATASET_NAME + \".tgz\"\n",
    "LABELS_FILENAME = DATASET_NAME + \".labels\"\n",
    "LABELS_COLOR = \"0x00FF00FF\"\n",
    "DATA_DIR = DATASET_NAME + \"/\"\n",
    "\n",
    "# Model and training settings\n",
    "MODEL_NAME = \"yolov5m\"\n",
    "MODEL_INPUT_PIXEL_SIZE = 640\n",
    "if SAMPLE_ONLY:\n",
    "    TRAINING_EPOCHS = 100\n",
    "else:\n",
    "    TRAINING_EPOCHS = 250 # This probably needs to be higher but for demo purposes it's ok."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca71cfc-ae7d-4152-bcb6-3b3b4e42b6b8",
   "metadata": {},
   "source": [
    "# Download and prepare the dataset\n",
    "\n",
    "The dataset used for this project is: Caltech-UCSD Birds-200-2011 (CUB-200-2011)  \n",
    "More information on this dataset can be found [here](https://www.vision.caltech.edu/datasets/cub_200_2011/).\n",
    "\n",
    "To prepare the dataset for training use:\n",
    "- Several text files are parsed for image data and combined into a DataFrame\n",
    "- Using the `training` field, the images are split into different folders for training and validation\n",
    "- A dataset configuration file under the `datasets/` folder is created to describe where the images are and the related class names\n",
    "- A labels file containing class data is generated for use on the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec926769-94f5-4f86-8db0-cb0cecaf0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "![ -z \"$MODEL_NAME\" ] && echo \"ERROR!! No model settings re-run \\\"Configure settings\\\" step above!\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def convert_coco_to_yolo(img_size, bbox):\n",
    "    x_center = (2*bbox[0] + bbox[2])/(2*img_size[0])\n",
    "    y_center = (2*bbox[1] + bbox[3])/(2*img_size[1])\n",
    "    width = bbox[2]/img_size[0]\n",
    "    height = bbox[3]/img_size[1]\n",
    "    return (round(x_center, 6), round(y_center, 6), round(width, 6), round(height, 6))\n",
    "\n",
    "def append_file(filename, line):\n",
    "    with open(filename, \"a\") as file:\n",
    "        file.write(line)\n",
    "        file.close()\n",
    "\n",
    "print(\"Downloading CUB_200_2011 files ...\")\n",
    "![ ! -f \"$DATASET_FILENAME\" ] && [ ! -d \"$DATA_DIR\" ] && wget --no-check-certificate -q -O $DATASET_FILENAME https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1\n",
    "\n",
    "print(\"Extracting CUB_200_2011 files ...\")\n",
    "# Unzip and cleanup the old compressed file\n",
    "![ ! -d \"$DATA_DIR\" ] && tar -xf $DATASET_FILENAME\n",
    "\n",
    "print(\"Clearing old configured dataset flles ...\")\n",
    "# Remove data archive\n",
    "!rm -rf $DATASET_FILENAME\n",
    "\n",
    "# read main list of images\n",
    "df = pd.read_csv(DATA_DIR + \"images.txt\", sep=' ',\n",
    "                 names=[\"id\", \"filepath\"])\n",
    "# merge list of image_id to class labels\n",
    "df = df.merge(pd.read_csv(DATA_DIR + \"image_class_labels.txt\", sep=' ',\n",
    "                          names=[\"id\", \"class_id\"]), on=\"id\")\n",
    "# merge list of image_id to training flag\n",
    "df = df.merge(pd.read_csv(DATA_DIR + \"train_test_split.txt\", sep=' ',\n",
    "                          names=[\"id\", \"training\"]), on=\"id\")\n",
    "# merge list of image_id to bounding box data\n",
    "df = df.merge(pd.read_csv(DATA_DIR + \"bounding_boxes.txt\", sep=' ',\n",
    "                          names=[\"id\", \"x_min\", \"y_min\", \"width\", \"height\"]), on=\"id\")\n",
    "\n",
    "classes_df = pd.read_csv(DATA_DIR + \"classes.txt\", sep=' ',\n",
    "                         names=[\"class_id\", \"class_name\"])\n",
    "df = df.merge(classes_df, on=\"class_id\")\n",
    "\n",
    "print(\"Generating dataset and label files ...\")\n",
    "\n",
    "# Create dataset folders\n",
    "!rm -rf datasets/\n",
    "!mkdir -p datasets/$DATASET_NAME/images/export\n",
    "!mkdir -p datasets/$DATASET_NAME/images/test\n",
    "!mkdir -p datasets/$DATASET_NAME/images/train\n",
    "!mkdir -p datasets/$DATASET_NAME/images/val\n",
    "!mkdir -p datasets/$DATASET_NAME/labels/export\n",
    "!mkdir -p datasets/$DATASET_NAME/labels/train\n",
    "!mkdir -p datasets/$DATASET_NAME/labels/val\n",
    "\n",
    "# Remove old labels file\n",
    "!rm -rf $LABELS_FILENAME\n",
    "\n",
    "# copy the dataset template\n",
    "!cp CUB_200_2011.yaml.template datasets/CUB_200_2011.yaml\n",
    "\n",
    "class_ids = sorted(df['class_id'].drop_duplicates())\n",
    "\n",
    "class_counter = -1\n",
    "for c_id in class_ids:\n",
    "    if len(CLASS_FILTER) == 0 or c_id in CLASS_FILTER:\n",
    "        class_counter += 1\n",
    "\n",
    "        # Parse the class name\n",
    "        c_name = classes_df[classes_df['class_id'] == c_id]['class_name'].array[0].split(\".\")[1]\n",
    "        print(f\"Parsing: {c_name}\")\n",
    "        # append to the dataset config\n",
    "        append_file(f\"datasets/{DATASET_NAME}.yaml\", f\"  {class_counter}: {c_name}\\n\")\n",
    "        # append to the labels file\n",
    "        append_file(LABELS_FILENAME, f'(structure)\"{c_name.replace(\" \", \"-\").replace(\"_\", \"-\").lower()},id=(guint)0x{class_counter:0>4X},color=(guint){LABELS_COLOR};\"\\n')\n",
    "\n",
    "        for image_id in df[df['class_id'] == c_id]['id']:\n",
    "            image_dfs = df[df['id'] == image_id]\n",
    "            filepath_orig = image_dfs['filepath'].array[0]\n",
    "            filename_new = filepath_orig.split(\"/\")[1]\n",
    "            label_filename = filename_new.split(\".\")[0] + \".txt\"\n",
    "\n",
    "            # convert from bounding box data to YOLO style x_center,y_center,w,h box\n",
    "            img_size = Image.open(f\"{DATASET_NAME}/images/{filepath_orig}\").size\n",
    "            bbox = (image_dfs['x_min'].array[0], image_dfs['y_min'].array[0], image_dfs['width'].array[0], image_dfs['height'].array[0])\n",
    "            yolo_box = convert_coco_to_yolo(img_size, bbox)\n",
    "\n",
    "            if image_dfs['training'].array[0] == 1:\n",
    "                loc = \"train\"\n",
    "            if image_dfs['training'].array[0] == 0:\n",
    "                loc = \"val\"\n",
    "\n",
    "            # TODO: copy -> rename\n",
    "            shutil.copy(f\"{DATASET_NAME}/images/{filepath_orig}\", f\"datasets/{DATASET_NAME}/images/{loc}/{filename_new}\")\n",
    "            # Create label file: <class_id> <x_center> <y_center> <width> <height>\n",
    "            with open(f\"datasets/{DATASET_NAME}/labels/{loc}/{label_filename}\", \"w\") as label_file:\n",
    "                label_file.write(f\"{class_counter} {yolo_box[0]} {yolo_box[1]} {yolo_box[2]} {yolo_box[3]}\\n\")\n",
    "\n",
    "# For SAMPLE_ONLY we use the limited val folder data as calibration for export/quantize operations\n",
    "# For !SAMPLE_ONLY the val data is too large: create export dir with only 1 calibration image per class type to keep the upload package smaller\n",
    "if not SAMPLE_ONLY:\n",
    "    # Setup export directory for calibration data\n",
    "    print(\"Generate calibration data ...\")\n",
    "    src_images_dir = f\"datasets/{DATA_DIR}/images/val/\"\n",
    "    dst_images_dir = f\"datasets/{DATA_DIR}/images/export/\"\n",
    "    src_labels_dir = f\"datasets/{DATA_DIR}/labels/val/\"\n",
    "    dst_labels_dir = f\"datasets/{DATA_DIR}/labels/export/\"\n",
    "    last_class = \"\"\n",
    "    translation_table = {ord(char): None for char in \"_0123456789\"}\n",
    "    for image_path in os.listdir(src_images_dir):\n",
    "        if image_path.endswith(\".jpg\"):\n",
    "            unique_name = image_path.translate(translation_table).lower()\n",
    "            if unique_name != last_class:\n",
    "                last_class = unique_name\n",
    "                shutil.copy(os.path.join(src_images_dir, image_path), dst_images_dir)\n",
    "                shutil.copy(os.path.join(src_labels_dir, image_path.replace(\".jpg\", \".txt\")), dst_labels_dir)\n",
    "\n",
    "    # Copy the dataset config to make an \"export\" version which points \"val\" dir to \"export\"\n",
    "    export_file = open(f\"datasets/{DATASET_NAME}.yaml\",\"r\")\n",
    "    contents = export_file.read()\n",
    "    export_file.close()\n",
    "    contents = contents.replace(\"val: images/val\", \"val: images/export\")\n",
    "    export_file = open(f\"datasets/{DATASET_NAME}-export.yaml\",\"w\")\n",
    "    export_file.write(contents)\n",
    "    export_file.close()\n",
    "else:\n",
    "    shutil.copy(f\"datasets/{DATASET_NAME}.yaml\", f\"datasets/{DATASET_NAME}-export.yaml\")\n",
    "\n",
    "# Download test images\n",
    "!wget --no-check-certificate -q -O datasets/$DATASET_NAME/images/test/multi-goldfinch-1.jpg https://t3.ftcdn.net/jpg/01/44/64/36/500_F_144643697_GJRUBtGc55KYSMpyg1Kucb9yJzvMQooW.jpg\n",
    "!wget --no-check-certificate -q -O datasets/$DATASET_NAME/images/test/northern-flicker-1.jpg https://upload.wikimedia.org/wikipedia/commons/5/5c/Northern_Flicker_%28Red-shafted%29.jpg\n",
    "!wget --no-check-certificate -q -O datasets/$DATASET_NAME/images/test/northern-cardinal-1.jpg https://cdn.pixabay.com/photo/2013/03/19/04/42/bird-94957_960_720.jpg\n",
    "!wget --no-check-certificate -q -O datasets/$DATASET_NAME/images/test/blue-jay-1.jpg https://cdn12.picryl.com/photo/2016/12/31/blue-jay-bird-feather-animals-b8ee04-1024.jpg\n",
    "!wget --no-check-certificate -q -O datasets/$DATASET_NAME/images/test/hummingbird-1.jpg http://res.freestockphotos.biz/pictures/17/17875-hummingbird-close-up-pv.jpg\n",
    "\n",
    "# Remove the original data now that we've created the dataset\n",
    "!rm -rf $DATA_DIR\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d44ded-5bbf-4729-a60d-6b53dd1e09bd",
   "metadata": {},
   "source": [
    "# Create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f13f0-323d-454f-888c-73c1979e81a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "![ -z \"$MODEL_NAME\" ] && echo \"ERROR!! No model settings re-run \\\"Configure settings\\\" step above!\"\n",
    "\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "\n",
    "create_model = YOLO()\n",
    "create_model._new(f\"{MODEL_NAME}.yaml\", task=\"detect\", verbose=True)\n",
    "create_model.save(f\"{MODEL_NAME}_train.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22285f0-1b4f-4d7e-b924-9f31a337102a",
   "metadata": {},
   "source": [
    "# Train and validate our model\n",
    "\n",
    "This step can be re-run several times and combined with the next step to test inference.\n",
    "\n",
    "If the `runs/` directory is removed (for cleaning), be sure to:\n",
    "- Restart the instance kernel\n",
    "- Re-run the \"Configure settings\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a86d11-78b5-4bc7-b323-b8dbade13f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "![ -z \"$MODEL_NAME\" ] && echo \"ERROR!! No model settings re-run \\\"Configure settings\\\" step above!\"\n",
    "\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load our model\n",
    "train_model = YOLO(f\"{MODEL_NAME}_train.pt\")\n",
    "\n",
    "# Train the model on the CUB-200-2011 dataset\n",
    "results = train_model.train(data=f\"datasets/{DATASET_NAME}.yaml\",\n",
    "                            epochs=TRAINING_EPOCHS,\n",
    "                            imgsz=MODEL_INPUT_PIXEL_SIZE)\n",
    "\n",
    "# Backup best.pt after training\n",
    "shutil.copy(f\"{results.save_dir}/weights/best.pt\", f\"{MODEL_NAME}_train.pt\")\n",
    "print(f\"Saved best weights as: {MODEL_NAME}_train.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1331a86b-d2fd-4ce9-a554-ef2f06b6c6e8",
   "metadata": {},
   "source": [
    "# Run inference using test images and unquantized model\n",
    "\n",
    "The results of the image tests are stored under the `tests/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122356e5-85a2-4080-a32b-fb314ffb66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "![ -z \"$MODEL_NAME\" ] && echo \"ERROR!! No model settings re-run \\\"Configure settings\\\" step above!\"\n",
    "\n",
    "# Clean up previous tests\n",
    "!rm -rf tests\n",
    "!mkdir -p tests\n",
    "\n",
    "import os\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load our model\n",
    "test_model = YOLO(f\"{MODEL_NAME}_train.pt\", task=\"detect\")\n",
    "\n",
    "# Run inference with the model on the test images\n",
    "directory = f\"datasets/{DATASET_NAME}/images/test/\"\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        f = os.path.join(directory, filename)\n",
    "        results = test_model(f)\n",
    "        for r in results:\n",
    "            r.save(filename=f\"tests/{filename}\")  # save to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5862bd-5a20-42ee-b991-457516336eff",
   "metadata": {},
   "source": [
    "# Compile and quantize on AI Hub\n",
    "\n",
    "Using our newly trained model:\n",
    "- Send the model to AI Hub to compile into an ONNX format\n",
    "- Use the val images to calibrate the compiled ONNX model\n",
    "- Send the model to AI Hub to quantize (int8)\n",
    "- Send the quantized model to AI Hub to compile as a TFLite binary for use on the device\n",
    "\n",
    "**Warning: You need to have saved your API Token in the `aihub_api_token.txt` file before proceeding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b67405-c270-4b84-8f0d-322a9b1b0b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "![ -z \"$MODEL_NAME\" ] && echo \"ERROR!! No model settings re-run \\\"Configure settings\\\" step above!\"\n",
    "![ ! -s \"aihub_api_token.txt\" ] && echo \"ERROR!! Place your AI Hub token in aihub_api_token.txt file and re-run \\\"Setup dependencies\\\"!\"\n",
    "\n",
    "# Based on AI Hub docs: https://app.aihub.qualcomm.com/docs/hub/quantize_examples.html\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import qai_hub as hub\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"Load and export our model with a traced graph representation ...\")\n",
    "# 1. Load our model and export as torchvision\n",
    "torch_model = YOLO(f\"{MODEL_NAME}_train.pt\")\n",
    "torch_model.export()\n",
    "\n",
    "print(\"Copy model to *.pth ...\")\n",
    "# 2. Copy to model.pth for torch compatibility\n",
    "shutil.copy(f\"{MODEL_NAME}_train.torchscript\", f\"{MODEL_NAME}_train.pth\")\n",
    "\n",
    "print(\"Start compile of model to ONNX format ...\")\n",
    "# 3. Compile the model on AI Hub to ONNX\n",
    "# Can also watch progress here: https://app.aihub.qualcomm.com/jobs/?type=compile\n",
    "pt_model = torch.load(f\"{MODEL_NAME}_train.pth\")\n",
    "input_shape = (1, 3, MODEL_INPUT_PIXEL_SIZE, MODEL_INPUT_PIXEL_SIZE)\n",
    "device = hub.Device(\"RB3 Gen 2 (Proxy)\")\n",
    "compile_onnx_job = hub.submit_compile_job(\n",
    "    name=\"DetectionModel_PyTorch_to_ONNX\",\n",
    "    model=pt_model,\n",
    "    device=device,\n",
    "    input_specs=dict(image_tensor=input_shape),\n",
    "    options=\"--target_runtime onnx\",\n",
    ")\n",
    "assert isinstance(compile_onnx_job, hub.CompileJob)\n",
    "\n",
    "unquantized_onnx_model = compile_onnx_job.get_target_model()\n",
    "assert isinstance(unquantized_onnx_model, hub.Model)\n",
    "print(\"Done!\")\n",
    "\n",
    "print(f\"Save model locally:{MODEL_NAME}_train_compiled.onnx\")\n",
    "# 3a. Save compiled ONNX model\n",
    "unquantized_onnx_model.download(f\"{MODEL_NAME}_train_compiled.onnx\")\n",
    "\n",
    "# 4. Load and pre-process downloaded calibration data\n",
    "sample_inputs = []\n",
    "\n",
    "print(\"Generate calibration data ...\")\n",
    "if SAMPLE_ONLY:\n",
    "    images_dir = f\"datasets/{DATA_DIR}/images/val/\"\n",
    "else:\n",
    "    images_dir = f\"datasets/{DATA_DIR}/images/export/\"\n",
    "for image_path in os.listdir(images_dir):\n",
    "    if image_path.endswith(\".jpg\"):\n",
    "        image = Image.open(os.path.join(images_dir, image_path))\n",
    "        print(f\"Converting {image_path} from to RGB ...\")\n",
    "        image = image.convert(\"RGB\").resize(input_shape[2:])\n",
    "        sample_input = np.array(image).astype(np.float32) / 255.0\n",
    "        sample_input = np.expand_dims(np.transpose(sample_input, (2, 0, 1)), 0)\n",
    "        sample_inputs.append(sample_input)\n",
    "calibration_data = dict(image_tensor=sample_inputs)\n",
    "\n",
    "# 5. Quantize the model\n",
    "# Can also watch progress here: https://app.aihub.qualcomm.com/jobs/?type=quantize\n",
    "print(\"Start quantize job at AI Hub for ONNX model ...\")\n",
    "quantize_job = hub.submit_quantize_job(\n",
    "    name=\"DetectionModel_Quantize_ONNX\",\n",
    "    model=unquantized_onnx_model,\n",
    "    calibration_data=calibration_data,\n",
    "    weights_dtype=hub.QuantizeDtype.INT8,\n",
    "    activations_dtype=hub.QuantizeDtype.INT8,\n",
    ")\n",
    "\n",
    "quantized_onnx_model = quantize_job.get_target_model()\n",
    "assert isinstance(quantized_onnx_model, hub.Model)\n",
    "print(\"Done!\")\n",
    "\n",
    "print(f\"Save model locally:{MODEL_NAME}_train_int8.onnx\")\n",
    "# 5a. Save quantized ONNX model\n",
    "quantized_onnx_model.download(f\"{MODEL_NAME}_train_int8.onnx\")\n",
    "\n",
    "print(\"Start compile of quantized model to TFLITE format ...\")\n",
    "# 6. Compile to target runtime (TFLite)\n",
    "# Can also watch progress here: https://app.aihub.qualcomm.com/jobs/?type=compile\n",
    "compile_tflite_job = hub.submit_compile_job(\n",
    "    name=\"DetectionModel_ONNX_to_TFLite\",\n",
    "    model=quantized_onnx_model,\n",
    "    device=device,\n",
    "    options=\"--target_runtime tflite --quantize_io --quantize_io_type int8 --force_channel_last_input image_tensor --force_channel_last_output output_0\",\n",
    ")\n",
    "assert isinstance(compile_tflite_job, hub.CompileJob)\n",
    "\n",
    "quantized_tflite_model = compile_tflite_job.get_target_model()\n",
    "assert isinstance(quantized_tflite_model, hub.Model)\n",
    "print(\"Done!\")\n",
    "\n",
    "print(f\"Save model locally:{MODEL_NAME}_train_int8.tflite\")\n",
    "# 6. Save tflite model\n",
    "quantized_tflite_model.download(f\"{MODEL_NAME}_train_int8.tflite\")\n",
    "\n",
    "print(\"DONE!  Please be sure to download the following files for use on the device:\")\n",
    "print(f\"- {MODEL_NAME}_train_int8.tflite\")\n",
    "print(f\"- {LABELS_FILENAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc70ad-63d8-4e61-af4e-c09d0e349327",
   "metadata": {},
   "source": [
    "# Run inference using test images and quantized model\n",
    "\n",
    "The results of the image tests are stored under the `tests_int8/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1748c46-958b-411c-8d22-9f943e1d8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "![ -z \"$MODEL_NAME\" ] && echo \"ERROR!! No model settings re-run \\\"Configure settings\\\" step above!\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Clean up previous tests\n",
    "!rm -rf tests_int8\n",
    "!mkdir -p tests_int8\n",
    "\n",
    "# Export torch model as tflite\n",
    "# 1. Load our model and export as torchvision\n",
    "#torch_model = YOLO(f\"{MODEL_NAME}_train.pt\")\n",
    "#torch_model.export(format=\"tflite\", data=f\"datasets/{DATASET_NAME}-export.yaml\",\n",
    "#                   imgsz=MODEL_INPUT_PIXEL_SIZE, int8=True)\n",
    "\n",
    "# Load our model\n",
    "interpreter = tf.lite.Interpreter(model_path=f\"{MODEL_NAME}_train_int8.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input = interpreter.get_input_details()[0]\n",
    "print(f\"input index:{input['index']}\")\n",
    "print(f\"({input['shape'][1]},{input['shape'][2]})\")\n",
    "output = interpreter.get_output_details()[0]\n",
    "print(output)\n",
    "\n",
    "# Run inference with the model on the test images\n",
    "directory = f\"datasets/{DATASET_NAME}/images/test/\"\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        f = os.path.join(directory, filename)\n",
    "        print(f\"Performing detection on: {filename}\")\n",
    "\n",
    "        # Reshape the input to match the model (640x640 by default)\n",
    "        image = Image.open(f)\n",
    "        image = image.resize((input['shape'][1], input['shape'][2]))\n",
    "        image = np.array(image).astype(np.int8)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        interpreter.set_tensor(input['index'], image)\n",
    "        interpreter.invoke()\n",
    "        print(interpreter.get_tensor(output['index']).reshape(-1, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c6046-23ef-4d72-b761-56378d7c2e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
